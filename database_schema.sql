-- Database Schema for ChartZ-AI
-- PostgreSQL schema for user management and dynamic CSV data storage
-- Enhanced with VMind-inspired AI-powered field detection and error recovery capabilities

-- Create extension for UUID generation
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- User Profile Management
CREATE TABLE user_profiles (
    user_id VARCHAR(128) PRIMARY KEY, -- Firebase UID
    email VARCHAR(255) UNIQUE NOT NULL,
    display_name VARCHAR(255),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Dataset Metadata - tracks each CSV file uploaded
CREATE TABLE datasets (
    dataset_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id VARCHAR(128) NOT NULL REFERENCES user_profiles(user_id) ON DELETE CASCADE,
    original_filename VARCHAR(255) NOT NULL,
    s3_key VARCHAR(500) NOT NULL, -- full S3 path
    file_size_bytes BIGINT,
    row_count INTEGER,
    column_count INTEGER,
    table_name VARCHAR(255) NOT NULL, -- dynamically generated table name for this dataset
    upload_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    ingestion_status VARCHAR(50) DEFAULT 'pending', -- pending, processing, completed, failed
    ingestion_date TIMESTAMP WITH TIME ZONE,
    error_message TEXT,
    metadata JSONB -- store column names, types, sample data, etc.
);

-- Column Metadata - tracks each column in each dataset
CREATE TABLE dataset_columns (
    column_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    dataset_id UUID NOT NULL REFERENCES datasets(dataset_id) ON DELETE CASCADE,
    column_name VARCHAR(255) NOT NULL,
    column_index INTEGER NOT NULL, -- position in the CSV
    data_type VARCHAR(50) NOT NULL, -- TEXT, INTEGER, DECIMAL, DATE, BOOLEAN
    postgres_type VARCHAR(50) NOT NULL, -- actual PostgreSQL type used
    is_nullable BOOLEAN DEFAULT true,
    sample_values JSONB, -- array of sample values for preview
    unique_count INTEGER, -- number of unique values (for categorical data)
    min_value TEXT, -- for numeric/date columns
    max_value TEXT, -- for numeric/date columns
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    -- VMind-inspired enhancements for intelligent field detection
    field_role VARCHAR(50), -- 'dimension', 'measure', 'identifier', 'unknown'
    semantic_type VARCHAR(50), -- 'categorical', 'numerical', 'temporal', 'text', 'boolean'
    cardinality_ratio DECIMAL(5,4), -- unique_count/row_count for categorical detection
    contains_nulls_pct DECIMAL(5,2), -- percentage of null values
    field_stats JSONB, -- additional statistical information
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Chart Generation History - tracks charts generated from datasets
CREATE TABLE chart_generations (
    generation_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id VARCHAR(128) NOT NULL REFERENCES user_profiles(user_id) ON DELETE CASCADE,
    dataset_id UUID REFERENCES datasets(dataset_id) ON DELETE CASCADE,
    user_prompt TEXT NOT NULL,
    generated_chart_config JSONB, -- the Visx/D3 configuration
    chart_type VARCHAR(100), -- bar, line, scatter, pie, etc.
    columns_used JSONB, -- array of column names used
    generation_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    model_used VARCHAR(100) DEFAULT 'claude-3.5-sonnet',
    execution_time_ms INTEGER,
    was_successful BOOLEAN DEFAULT true,
    error_message TEXT,
    -- VMind-inspired enhancements for learning from successful patterns
    sql_query TEXT, -- the SQL used for data aggregation
    field_mappings JSONB, -- which fields mapped to which visual channels
    chart_complexity VARCHAR(20), -- 'simple', 'medium', 'complex'
    confidence_score DECIMAL(3,2), -- AI confidence in the chart choice
    generation_strategy VARCHAR(50), -- 'field_detection', 'llm_analysis', 'rule_based'
    data_quality_score DECIMAL(3,2), -- assessment of input data quality
    -- Phase 1 Dynamic Data Fetching fields
    generated_sql_query TEXT, -- The SQL query generated by AI for data aggregation
    sql_fields JSONB, -- Fields returned by SQL query (for field mapping validation)
    spec_fields JSONB, -- Fields expected by VChart spec (for field mapping validation) 
    fields_compatible BOOLEAN, -- Whether SQL fields match spec fields exactly
    dynamic_fetch_ready BOOLEAN DEFAULT false -- Whether this chart is ready for dynamic data fetching
);

-- VMind-inspired table for chart generation attempts (including failures for learning)
CREATE TABLE chart_generation_attempts (
    attempt_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    generation_id UUID NOT NULL REFERENCES chart_generations(generation_id) ON DELETE CASCADE,
    attempt_number INTEGER NOT NULL,
    step_name VARCHAR(100) NOT NULL, -- 'field_detection', 'sql_generation', 'chart_mapping', 'spec_building'
    step_input JSONB,
    step_output JSONB,
    error_message TEXT,
    correction_applied TEXT, -- what correction was made for retry
    was_successful BOOLEAN DEFAULT false,
    execution_time_ms INTEGER,
    attempt_timestamp TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(generation_id, attempt_number, step_name)
);

-- VMind-inspired table for chart knowledge and examples (for consistent templates)
CREATE TABLE chart_knowledge (
    knowledge_id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    chart_type VARCHAR(50) NOT NULL,
    knowledge_category VARCHAR(50) NOT NULL, -- 'template', 'example', 'rule', 'constraint'
    name VARCHAR(100) NOT NULL,
    description TEXT,
    data_requirements JSONB, -- min/max dimensions, measures, cardinality constraints
    vchart_template JSONB, -- standardized VChart specification template
    sample_data JSONB, -- example data structure
    use_cases TEXT[], -- array of applicable scenarios
    priority_score INTEGER DEFAULT 0, -- for selecting best template
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- Indexes for performance
CREATE INDEX idx_user_profiles_email ON user_profiles(email);
CREATE INDEX idx_datasets_user_id ON datasets(user_id);
CREATE INDEX idx_datasets_upload_date ON datasets(upload_date DESC);
CREATE INDEX idx_dataset_columns_dataset_id ON dataset_columns(dataset_id);
CREATE INDEX idx_chart_generations_user_id ON chart_generations(user_id);
CREATE INDEX idx_chart_generations_dataset_id ON chart_generations(dataset_id);
CREATE INDEX idx_chart_generations_date ON chart_generations(generation_date DESC);

-- VMind enhancement indexes
CREATE INDEX idx_chart_generation_attempts_generation_id ON chart_generation_attempts(generation_id);
CREATE INDEX idx_chart_generation_attempts_step_name ON chart_generation_attempts(step_name);
CREATE INDEX idx_chart_generation_attempts_timestamp ON chart_generation_attempts(attempt_timestamp DESC);
CREATE INDEX idx_chart_knowledge_type ON chart_knowledge(chart_type);
CREATE INDEX idx_chart_knowledge_category ON chart_knowledge(knowledge_category);
CREATE INDEX idx_dataset_columns_field_role ON dataset_columns(field_role);
CREATE INDEX idx_dataset_columns_semantic_type ON dataset_columns(semantic_type);

-- Function to generate unique table names for datasets
CREATE OR REPLACE FUNCTION generate_dataset_table_name(user_uuid VARCHAR, original_name VARCHAR)
RETURNS VARCHAR AS $$
DECLARE
    clean_name VARCHAR;
    table_name VARCHAR;
    counter INTEGER := 0;
    final_name VARCHAR;
BEGIN
    clean_name := regexp_replace(lower(original_name), '[^a-z0-9_]', '_', 'g');
    clean_name := regexp_replace(clean_name, '_+', '_', 'g');
    clean_name := trim(both '_' from clean_name);
    clean_name := regexp_replace(clean_name, '\.(csv|txt)$', '', 'i');
    
    clean_name := left(clean_name, 20);
    
    table_name := 'user_' || replace(user_uuid, '-', '_') || '_' || clean_name;
    
    final_name := table_name;
    WHILE EXISTS (
        SELECT 1 FROM datasets WHERE datasets.table_name = final_name
    ) LOOP
        counter := counter + 1;
        final_name := table_name || '_' || counter::text;
    END LOOP;
    
    RETURN final_name;
END;
$$ LANGUAGE plpgsql;

-- VMind-inspired function to calculate field statistics and roles
CREATE OR REPLACE FUNCTION analyze_field_characteristics(
    p_dataset_id UUID,
    p_table_name VARCHAR
) RETURNS TABLE (
    column_name VARCHAR,
    suggested_field_role VARCHAR,
    suggested_semantic_type VARCHAR,
    cardinality_ratio DECIMAL,
    null_percentage DECIMAL,
    field_statistics JSONB
) AS $$
DECLARE
    col_record RECORD;
    total_rows INTEGER;
    sql_query TEXT;
    stats_result RECORD;
BEGIN
    -- Get total row count
    EXECUTE format('SELECT COUNT(*) FROM %I', p_table_name) INTO total_rows;
    
    -- Loop through each column in the dataset
    FOR col_record IN 
        SELECT dc.column_name, dc.data_type, dc.postgres_type, dc.unique_count
        FROM dataset_columns dc 
        WHERE dc.dataset_id = p_dataset_id 
        ORDER BY dc.column_index
    LOOP
        -- Build dynamic query to get column statistics
        sql_query := format('
            SELECT 
                COUNT(DISTINCT %I) as distinct_count,
                COUNT(%I) as non_null_count,
                COUNT(*) as total_count,
                (COUNT(*) - COUNT(%I))::FLOAT / COUNT(*) * 100 as null_pct,
                MIN(%I::TEXT) as min_val,
                MAX(%I::TEXT) as max_val
            FROM %I
        ', col_record.column_name, col_record.column_name, col_record.column_name, 
           col_record.column_name, col_record.column_name, p_table_name);
        
        EXECUTE sql_query INTO stats_result;
        
        -- Calculate cardinality ratio
        column_name := col_record.column_name;
        cardinality_ratio := CASE 
            WHEN total_rows > 0 THEN stats_result.distinct_count::DECIMAL / total_rows 
            ELSE 0 
        END;
        null_percentage := stats_result.null_pct;
        
        -- Determine field role based on data characteristics
        suggested_field_role := CASE 
            -- Identifier detection (high uniqueness)
            WHEN cardinality_ratio > 0.95 THEN 'identifier'
            
            -- Dimension detection (categorical data)
            WHEN col_record.data_type IN ('TEXT') AND cardinality_ratio < 0.5 THEN 'dimension'
            WHEN col_record.data_type IN ('DATE') THEN 'dimension'
            WHEN col_record.postgres_type = 'boolean' THEN 'dimension'
            WHEN col_record.data_type IN ('TEXT') AND stats_result.distinct_count <= 50 THEN 'dimension'
            
            -- Measure detection (numerical data suitable for aggregation)
            WHEN col_record.data_type IN ('INTEGER', 'DECIMAL') AND cardinality_ratio > 0.1 THEN 'measure'
            WHEN col_record.data_type IN ('FLOAT', 'NUMERIC') THEN 'measure'
            
            ELSE 'unknown'
        END;
        
        -- Determine semantic type
        suggested_semantic_type := CASE
            WHEN col_record.data_type IN ('TEXT') AND cardinality_ratio < 0.2 THEN 'categorical'
            WHEN col_record.data_type IN ('INTEGER', 'DECIMAL', 'FLOAT', 'NUMERIC') THEN 'numerical'
            WHEN col_record.data_type = 'DATE' OR col_record.postgres_type LIKE '%timestamp%' THEN 'temporal'
            WHEN col_record.postgres_type = 'boolean' THEN 'boolean'
            ELSE 'text'
        END;
        
        -- Build field statistics JSON
        field_statistics := json_build_object(
            'distinct_count', stats_result.distinct_count,
            'non_null_count', stats_result.non_null_count,
            'null_percentage', stats_result.null_pct,
            'min_value', stats_result.min_val,
            'max_value', stats_result.max_val,
            'data_type', col_record.data_type,
            'postgres_type', col_record.postgres_type,
            'is_suitable_for_grouping', (suggested_field_role = 'dimension'),
            'is_suitable_for_aggregation', (suggested_field_role = 'measure')
        );
        
        RETURN NEXT;
    END LOOP;
    
    RETURN;
END;
$$ LANGUAGE plpgsql;

-- VMind-inspired function to update field analysis for a dataset
CREATE OR REPLACE FUNCTION update_field_analysis(p_dataset_id UUID, p_table_name VARCHAR)
RETURNS VOID AS $$
DECLARE
    field_record RECORD;
BEGIN
    -- Update field characteristics for all columns in the dataset
    FOR field_record IN 
        SELECT * FROM analyze_field_characteristics(p_dataset_id, p_table_name)
    LOOP
        UPDATE dataset_columns 
        SET 
            field_role = field_record.suggested_field_role,
            semantic_type = field_record.suggested_semantic_type,
            cardinality_ratio = field_record.cardinality_ratio,
            contains_nulls_pct = field_record.null_percentage,
            field_stats = field_record.field_statistics,
            updated_at = CURRENT_TIMESTAMP
        WHERE dataset_id = p_dataset_id 
        AND column_name = field_record.column_name;
    END LOOP;
    
    RAISE NOTICE 'Field analysis updated for dataset %', p_dataset_id;
END;
$$ LANGUAGE plpgsql;

-- Insert standard pie chart knowledge base
INSERT INTO chart_knowledge (chart_type, knowledge_category, name, description, data_requirements, vchart_template, sample_data, use_cases, priority_score)
VALUES 
(
    'pie',
    'template',
    'basic_pie_chart',
    'Standard pie chart template for categorical data distribution',
    '{
        "min_dimensions": 1,
        "min_measures": 1,
        "max_categories": 15,
        "data_constraints": [
            "All values must be positive numbers",
            "Category field should have reasonable cardinality (2-15 unique values)",
            "No missing or null values in key fields"
        ]
    }'::jsonb,
    '{
        "type": "pie",
        "data": {
            "values": []
        },
        "categoryField": "category",
        "valueField": "value",
        "outerRadius": 0.8,
        "title": {
            "visible": true,
            "text": "Data Distribution"
        },
        "legends": {
            "visible": true,
            "orient": "right"
        },
        "label": {
            "visible": true
        },
        "tooltip": {
            "mark": {
                "content": [
                    {
                        "key": "datum => datum[\"category\"]",
                        "value": "datum => datum[\"value\"]"
                    }
                ]
            }
        }
    }'::jsonb,
    '[
        {"category": "Product A", "value": 30},
        {"category": "Product B", "value": 45},
        {"category": "Product C", "value": 25}
    ]'::jsonb,
    ARRAY['categorical_distribution', 'market_share', 'survey_results', 'basic_aggregation'],
    100
),
(
    'pie',
    'template', 
    'donut_chart',
    'Donut chart variation with inner radius for better readability',
    '{
        "min_dimensions": 1,
        "min_measures": 1,
        "max_categories": 12,
        "recommended_for": "larger_datasets"
    }'::jsonb,
    '{
        "type": "pie",
        "data": {
            "values": []
        },
        "categoryField": "category",
        "valueField": "value", 
        "outerRadius": 0.8,
        "innerRadius": 0.4,
        "title": {
            "visible": true,
            "text": "Distribution Analysis"
        },
        "legends": {
            "visible": true,
            "orient": "bottom"
        },
        "label": {
            "visible": true
        },
        "tooltip": {
            "mark": {
                "content": [
                    {
                        "key": "datum => datum[\"category\"]",
                        "value": "datum => datum[\"value\"]"
                    }
                ]
            }
        }
    }'::jsonb,
    '[
        {"category": "Region North", "value": 120},
        {"category": "Region South", "value": 95},
        {"category": "Region East", "value": 87},
        {"category": "Region West", "value": 110}
    ]'::jsonb,
    ARRAY['regional_analysis', 'department_breakdown', 'medium_cardinality'],
    90
);

-- Sample data for testing (optional)
-- INSERT INTO user_profiles (email, display_name) 
-- VALUES ('test@example.com', 'Test User');

COMMENT ON TABLE user_profiles IS 'Stores basic user account information';
COMMENT ON TABLE datasets IS 'Metadata about uploaded CSV files and their ingestion status';
COMMENT ON TABLE dataset_columns IS 'Schema information for each column in uploaded datasets with VMind-inspired field role detection';
COMMENT ON TABLE chart_generations IS 'History of chart generation requests and results with VMind-inspired learning capabilities';
COMMENT ON TABLE chart_generation_attempts IS 'Tracks all steps in chart generation workflow including failures for learning and debugging';
COMMENT ON TABLE chart_knowledge IS 'Stores chart templates, examples, and rules for consistent chart generation';
COMMENT ON FUNCTION analyze_field_characteristics IS 'Analyzes dataset columns to determine field roles and characteristics for intelligent chart generation';
COMMENT ON FUNCTION update_field_analysis IS 'Updates field analysis for all columns in a dataset, should be called after data ingestion';